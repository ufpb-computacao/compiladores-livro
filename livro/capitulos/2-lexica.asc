== Análise Léxica

:cap: cap2
:online: {gitrepo}/blob/{edition}/livro/capitulos/code/{cap}
:local: code/{cap}

.Objetivos do capítulo
____
Ao final deste capítulo você deverá ser capaz de:

* Entender a função do analisador léxico dentro de um compilador
* Descrever a estrutura léxica de uma linguagem usando expressões regulares
* Criar o analisador léxico para uma linguagem usando um gerador de analisadores
____

[NOTE]
.Organização prévia
====
Para acompanhar as explicações do capítulo é recomendado que você 
instale o software *flex* no seu computador, as intruções de instalação
se encontram no <<app_instalacao>>, na <<sec_install_flex>>.
====

A análise léxica é a primeira etapa do compilador, e recebe o arquivo de 
entrada criado pelo usuário. O arquivo de entrada é geralmente armazenado 
como uma sequência de caracteres individuais que podem ser lidos. A análise 
léxica tem como função agrupar os caracteres individuais em _((tokens))_, que 
são as menores unidades com significado no programa-fonte. Um _token_ pode 
ser pensado como sendo similar a uma palavra. 

Podemos fazer uma analogia do processo de análise do compilador com o ato 
de ler um texto. Na leitura, nós não lemos e decodificamos individualmente 
cada letra; nosso cérebro lê e processa um texto uma palavra por vez. Isso 
é comprovado pelo fato que conseguimos entender um texto mesmo que as 
palavras tenham erros de ortografia ou mesmo sejam escritas de maneira 
diferente. 

A análise léxica faz com que as etapas seguintes do compilador possam 
trabalhar no nível das palavras, ao invés do nível dos caracteres 
individuais. Isso facilita bastante o trabalho das etapas posteriores. 
Na análise léxica também são realizadas algumas tarefas como remover 
comentários dos arquivos do programa-fonte e registrar em uma tabela
os nomes de identificadores usados no programa. Os detalhes de como 
isso é feito são o tópico deste capítulo. 

=== O Funcionamento da Análise Léxica

(((Análise,Léxica)))

Como vimos, a análise léxica agrupa os caracteres do arquivo de entrada 
(que contém o programa-fonte) em _tokens_. Um _token_ é similar a uma 
palavra do texto de entrada e é composto por duas partes principais:

.  um _tipo_;
.  um _valor_ opcional. 

O tipo indica que espécie de ``palavra'' o _token_ pode representar: um número, 
um sinal de pontuação, um identificador (nome de variável ou função), etc. 
O valor é usado em alguns tipos de _tokens_ para armazenar alguma informação 
adicional necessária. Outras informações podem ser associadas a cada _token_, 
dependendo das necessidades do compilador. 
Um exemplo comum é a posição no arquivo de entrada (linha e coluna) onde 
o _token_ começa, o que ajuda no tratamento de erros. 

Um exemplo vai deixar essas ideias mais claras. Vamos usar uma 
linguagem simples para expressões aritméticas com operandos constantes, 
uma linguagem "de calculadora". Na linguagem são permitidos números inteiros 
(positivos ou negativos), parênteses, e as quatro operações aritméticas básicas, 
representadas pelos caracteres usuais:

* soma: `+`
* subtração: +-+
* multiplicação: +*+
* divisão: +/+

Os tipos de _((tokens))_ são: número, operador e pontuação (para representar os 
parênteses). Todos os três tipos precisam armazenar informação no campo de 
valor do _token_. Por exemplo, um _token_ do tipo número diz apenas que um 
número foi encontrado, e o valor do número é guardado no campo de valor 
do _token_. 

Um exemplo de programa nessa linguagem é o seguinte:

.Exemplo de expressão aritmética
----
42 + (675 * 31) - 20925
----

Neste exemplo, todos os _tokens_ de tipo número são formados por mais de 
um caractere, o maior tendo cinco caracteres (+20925+). O analisador 
léxico gera para esse exemplo a seguinte sequência de _tokens_:

[[tabela-seq-tokens]]
.Sequência de _tokens_ para o exemplo
[width="60%",cols="^m,^,^m", options="header", frame="topbot", grid="none"]
|====
|Lexema   | Tipo      | Valor 
|42       | Número    | 42
|`+`      | Operador  | SOMA
|+(+      | Pontuação | PARESQ
|675      | Número    | 675
|+*+      | Operador  | MULT
|31       | Número    | 31
|+)+      | Pontuação | PARDIR
|+-+      | Operador  | SUB
|20925    | Número    | 20925
|====

Um _((lexema))_ é a sequência de caracteres que dá origem a um _token_. No 
exemplo atual, o lexema +20925+ gera um _token_ de tipo número e 
valor 20925. Note que o lexema é um conjunto de caracteres, a _string_ 
+"20925"+, enquanto que o valor do _token_ é o valor numérico 20925. 

Os valores dos _tokens_ de tipo operador representam que operador gerou 
o _token_, e os valores do tipo pontuação funcionam da mesma forma. Os 
valores são escritos em letração do analisador léxico esses valores são representados por 
constantes numéricas. 

Para tornar esse exemplo mais concreto, vamos examinar a estrutura da 
implementação do analisador léxico para essa linguagem simples. 

==== Implementação Manual de um Analisador Léxico

(((Analisador Léxico)))

Para uma linguagem simples como a linguagem de expressões aritmética 
do exemplo, escrever um programa que faz a análise léxica não apresenta
grande dificuldade. Nesta seção vamos examinar as partes mais importantes
do analisador léxico para essa linguagem, pois vários elementos serão 
similares para linguagens mais complexas. 

O código fonte completo do analisador léxico para a linguagem de expressões 
pode ser encontrado no seguinte arquivo: 

:srcfile: exp_lexer.c

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

Aqui vamos analisar as principais partes deste programa. Começamos com a definição da 
estrutura que vai guardar os _tokens_:

[source, c]
.Definição de estrutura para _tokens_
----
typedef struct 
{
  int tipo;
  int valor;
} Token; 
----

Como vimos, um _token_ tem dois campos: o tipo do _token_ e um valor associado. Ambos 
os campos são inteiros, então definimos algumas constantes para representar os valores 
possíveis desses campos. As primeiras constantes especificam o tipo de _token_:

[source, c]
.Constantes que representam o tipo do _token_
----
#define TOK_NUM         0 
#define TOK_OP          1
#define TOK_PONT        2
----

Com relação ao valor, para números o valor do _token_ é apenas o valor do
número encontrado. Para operadores e pontuação, por outro lado, precisamos 
apenas de alguns valores para representar os quatro operadores e dois caracteres
de pontuação:

[source, c]
.Constantes para operadores e pontuação
----
#define SOMA            0
#define SUB             1
#define MULT            2
#define DIV             3

#define PARESQ          0
#define PARDIR          1
----

O código do analisador léxico usa algumas variáveis globais, para facilitar o 
entendimento. O programa funciona recebendo um programa de entrada como uma 
_string_ (normalmente um compilador recebe o programa de entrada em um arquivo).
As informações guardadas em variáveis globais são a _string_ 
contendo o código do programa de entrada, o tamanho dessa _string_ e a posição 
atual da análise dentro da _string_:

[source, c]
.Variáveis globais para guardar o estado da análise
----
// string que contem o codigo que esta em analise
char *codigo;

// tamanho da string com o codigo
int tamanho;

// guarda posicao atual no codigo
int pos;
----

A análise é iniciada ao chamar a função +inicia_analise+, que estabelece o 
valor inicial das variáveis globais:

[source, c]
.Função para inicializar a análise léxica
----
void inicializa_analise(char *prog)
{
  codigo = prog;
  tamanho = strlen(codigo);
  pos = 0;
}
----

A função +inicia_analise+ recebe uma _string_ contendo o código do programa 
de entrada como parâmetro (+prog+), e armazena um ponteiro para essa _string_
na variável global +codigo+; a função também estabelece o valor da variável 
global +tamanho+ e inicializa a posição atual na análise com valor zero. 

A análise léxica em si funciona de maneira incremental: ao invés de analisar 
todo o código de entrada de uma vez e retornar todo o fluxo de _tokens_, a 
função de análise retorna um _token_ de cada vez. Por isso, o nome da função 
que realiza a análise é +proximo_token+, e ela retorna o próximo _token_ na 
sequência a cada vez que é chamada. 

Vamos analisar a função +proximo_token+ por partes. Começando pelas variáveis
locais usadas pela função: 

[source, c]
.Função que realiza a análise léxica
----
Token *proximo_token(Token *tok)
{
  char c;
  char valor[200];    // string para obter valor de um numero
  int  vpos = 0;      // posicao na string de valor
----

Como indicado nos comentários, a _string_ +valor+ é usada para determinar o 
valor de um _token_ de tipo número. Isso é necessário porque a função de análise 
lê um caractere do número de cada vez; a variável +vpos+ é usada para guardar a 
posição atual na string +valor+. A variável +c+, de tipo caractere, guarda o 
caractere atualmente sendo lido do código de entrada. 

Na maioria das linguagens de programação, os espaços em branco não são 
significativos para o programa, e portanto o programador pode usar quantidades 
variáveis de espaço entre os _tokens_ do programa. Por isso, a primeira tarefa 
do analisador é pular todo o espaço em branco que for necessário para chegar até 
o primeiro caractere do próximo _token_. Isso é feito pela seguinte parte da 
função +proximo_token+:

[source, c]
.Código para pular o espaço em branco antes do próximo _token_
----
  c = le_caractere();
  while (isspace(c)) {
    c = le_caractere();
  }
----

A função +le_caractere+ é uma função auxiliar que obtém o próximo caractere do 
programa de entrada, atualizando a posição atual dentro da _string_ que contém 
o programa (para detalhes, veja o código-fonte completo do analisador). O código 
acima lê caracteres da entrada enquanto eles os caracteres lidos forem de espaço 
(espaço em branco, tabulação e caracteres de nova linha), usando a função 
+isspace+ da biblioteca padrão da linguagem C. Ao final desse _loop_, 
a variável +c+ vai conter o primeiro caractere do próximo _token_. 

A função de 
análise deve então usar esse caractere para determinar que tipo de _token_ está 
sendo lido, e continuar de acordo com o tipo. Nessa linguagem é possível 
determinar o tipo do _token_ olhando apenas para o seu primeiro caractere, mas 
em linguagens mais complexas isso geralmente não é possível. 

Se o primeiro caractere do _token_ for um dígito, a análise determina que o 
próximo _token_ é um número. O processo a seguir é ler os próximos caracteres 
enquanto forem dígitos, armazenando cada dígito lido na _string_ auxiliar 
+valor+. Ao final, o valor do _token_ é obtido através da conversão da 
string +valor+ para um número inteiro, usando a função +atoi()+:

[source, c]
.Leitura de um _token_ de tipo número
----
  if (isdigit(c)) {
    tok->tipo = TOK_NUM;
    valor[vpos++] = c;
    c = le_caractere();
    while (isdigit(c)) {
      valor[vpos++] = c;
      c = le_caractere();
    }
    // retorna o primeiro caractere que nao eh um digito
    // para ser lido como parte do proximo token
    pos--;
    // termina a string de valor com um caractere 0
    valor[vpos] = '\0';
    // converte string de valor para numero
    tok->valor = atoi(valor);
  }
----

Se o primeiro caractere não for um dígito, a análise testa se é um caractere 
de operador e, se for, apenas determina qual constante deve ser usada como 
valor do _token_:

[source, c]
.Leitura de um _token_ operador
----
  else if (strchr(ops, c) != NULL) {
    tok->tipo  = TOK_OP;
    tok->valor = operador(c);
  }
----

A condição no +if+ acima é uma forma mais curta de verificar se o caractere é um 
dos operadores, ao invés de usar quatro comparações. A constante global +ops+ é 
definida da seguinte forma:

[source, c]
.Conjunto de operadores da linguagem
----
const char *ops = "+-*/";
----

E a função +strchr+ da biblioteca padrão da linguagem C retorna um valor diferente 
de +NULL+ se o caractere +c+ faz parte da _string_ +ops+. A função auxiliar 
+operador+ apenas associa o caractere do operador com a constante numérica 
correspondente; por exemplo se +c+ for o caractere `+`, a função +operador+ 
retorna a constante +SOMA+. A definição da função +operador+ pode ser vista 
no código-fonte completo do analisador. 

A última possibilidade de _token_ válido para essa linguagem ocorre se o 
primeiro caractere for um parêntese. Nesse caso o tipo do _token_ é determinado 
como pontuação e o valor é a constante +PARESQ+ se o caractere lido foi +(+ e 
+PARDIR+ se o caractere lido foi +)+. Se o caractere não foi nenhuma das 
possibilidades anteriores (dígito, operador ou parêntese) a análise retorna o 
valor +NULL+ para indicar uma falha na análise. Isso pode ocorrer porque foi 
encontrado na entrada um caractere que não pertence à linguagem, ou porque a 
entrada chegou ao fim. Se não ocorreu uma falha de análise, a função deve 
retornar o _token_ que foi obtido da análise. Isso nos leva ao final da função 
+proximo_token+:

[source, c]
.Final da função de análise léxica
----
  else if (c == '(' || c == ')') {
    tok->tipo  = TOK_PONT;
    tok->valor = (c == '(' ? PARESQ : PARDIR);
  }
  else
    return NULL;

  return tok;
}
---- 

A função +proximo_token+ completa, reunindo os trechos vistos de forma 
separada, pode ser vista a seguir:

[source, c]
.Função completa que faz a análise léxica
----
Token *proximo_token(Token *tok)
{
  char c;
  char valor[200];    // string para obter valor de um numero
  int  vpos = 0;      // posicao na string de valor

  c = le_caractere();
  // pula todos os espacos em branco
  while (isspace(c)) {
    c = le_caractere();
  }

  if (isdigit(c)) {
    tok->tipo = TOK_NUM;
    valor[vpos++] = c;
    c = le_caractere();
    while (isdigit(c)) {
      valor[vpos++] = c;
      c = le_caractere();
    }
    // retorna o primeiro caractere que nao eh um digito
    // para ser lido como parte do proximo token
    pos--;
    // termina a string de valor com um caractere 0
    valor[vpos] = '\0';
    // converte string de valor para numero
    tok->valor = atoi(valor);
  }
  else if (strchr(ops, c) != NULL) {
    tok->tipo  = TOK_OP;
    tok->valor = operador(c);
  }
  else if (c == '(' || c == ')') {
    tok->tipo  = TOK_PONT;
    tok->valor = (c == '(' ? PARESQ : PARDIR);
  }
  else
    return NULL;

  return tok;
}
----

O código completo do analisador inclui algumas funções de impressão e uma 
função principal que lê o programa de entrada a partir do teclado e mostra 
a sequência de _tokens_ obtida desta entrada. A função principal é:

[source, c]
.Função principal do programa de análise léxica
----
int main(void)
{
  char  entrada[200];
  Token tok;

  printf("Analise Lexica para Expressoes\n");

  printf("Expressao: ");
  fgets(entrada, 200, stdin);

  inicializa_analise(entrada);

  printf("\n===== Analise =====\n");

  while (proximo_token(&tok) != NULL) {
    imprime_token(&tok);
  }

  printf("\n");

  return 0;
}
----

Executando esse programa para a expressão de exemplo que vimos anteriormente, 
obtemos a seguinte saida:

.Saída para a expressão 42 + (675 * 31) - 20925
----
Analise Lexica para Expressoes
Expressao: 42 + (675 * 31) - 20925

===== Analise =====
Tipo: Numero     -- Valor: 42
Tipo: Operador   -- Valor: SOMA
Tipo: Pontuacao  -- Valor: PARESQ
Tipo: Numero     -- Valor: 675
Tipo: Operador   -- Valor: MULT
Tipo: Numero     -- Valor: 31
Tipo: Pontuacao  -- Valor: PARDIR
Tipo: Operador   -- Valor: SUB
Tipo: Numero     -- Valor: 20925
----

A saída está de acordo com o que esperamos da análise léxica dessa linguagem, 
como pode ser visto na Tabela <<tabela-seq-tokens>>. 

Vimos que para uma linguagem simples como a de expressões, é fácil criar 
diretamente o analisador léxico necessário. Entretanto, à medida que a 
estrutura da linguagem se torna mais complexa (como ocorre nas linguagens de
programação real), a complexidade do analisador léxico vai crescendo e se 
torna difícil criar o analisador léxico sem ter alguma técnica sistemática 
para lidar com a complexidade. 

As técnicas que usaremos para isso são relacionadas a uma classe de linguagens 
formais conhecida como _linguagens regulares_. Essas técnicas são fundamentadas 
em uma teoria bem desenvolvida, e contam com ferramentas que automatizam a maior 
parte do processo de análise léxica. 

=== Linguagens Regulares e Expressões Regulares

(((Linguagens Regulares)))

As linguagens regulares são um tipo de linguagem formal que são frequentemente 
utilizadas para representar padrões simples de texto. Uma técnica de 
representação muito utilizada para as linguagens regulares são as chamadas 
_expressões regulares_. Bibliotecas que dão suporte ao uso de expressões 
regulares estão disponíveis na maioria das linguagens de programação e são
muito usadas para busca em textos e para validação de entrada textual (para 
formulários de entrada de dados, por exemplo). 

Uma outra técnica de representação usada para linguagens regulares são os 
_((autômatos finitos))_. Autômatos finitos e expressões regulares são equivalentes, 
ou seja, todo padrão que pode ser representado por uma técnica também pode ser 
representada pela outra. Os autômatos finitos podem ser utilizados para 
organizar os padrões léxicos de uma linguagem, facilitando a implementação 
direta de um analisador léxico para ela. Ou seja, com os autômatos finitos 
podemos criar analisadores léxicos para linguagens mais complexas, e de maneira 
mais sistemática e confiável do que vimos no exemplo da linguagem de expressões. 

Para criar um analisador léxico dessa forma devemos definir os autômatos finitos 
que representam os padrões associados a cada tipo de _token_, depois combinar 
esses autômatos em um único autômato, e então implementar o autômato finito 
resultante como um programa. Mais detalhes sobre como fazer isso podem ser 
encontrados em outros livros sobre compiladores, por exemplo o famoso 
``livro do dragão'' (Compiladores: Princípios, Técnicas e Ferramentas, 2^a^ 
edição, de Aho et al., editora Pearson/Addison-Wesley). 

Aqui vamos usar uma abordagem mais automatizada, criando analisadores léxicos 
a partir de ferramentas chamadas de _geradores de analisadores léxicos_. Esses 
geradores recebem como entrada uma especificação dos padrões que definem cada 
tipo de _token_, e criam na saída o código-fonte do analisador léxico. Criar 
analisadores usando um gerador é prático e temos um certo nível de garantia que 
o código gerado estará correto. Para usar um gerador, no entanto, é preciso saber 
como representar os padrões que definem tipos de _tokens_ da linguagem como 
expressões regulares. 

==== Expressões Regulares

(((Expressões Regulares)))

As expressões regulares descrevem padrões simples de texto de forma compacta 
e sem ambiguidade. Por exemplo, o padrão que descreve todas as _strings_ 
formadas com caracteres +a+ e +b+ que começam com +a+ e terminam com +b+ pode 
ser escrito como a expressão regular `a(a|b)*b` (a construção dessa expressão 
será explicada em breve). 

Existem várias sintaxes e representações diferentes para expressões regulares, 
dependendo da linguagem ou biblioteca utilizada. Como vamos utilizar o 
gerador de analisadores flex, usaremos aqui a sintaxe usada nessa 
ferramenta. 

===== Expressões básicas 

(((ER)))

Cada Expressão Regular (ER) é uma _string_ que representa um conjunto de 
_strings_; também podemos dizer que uma ER representa um _padrão_ que é 
satisfeito por um conjunto de _strings_. 

A maioria dos caracteres representam eles mesmos em uma expressão regular. 
Por exemplo, o caractere +a+ em uma ER representa o próprio caractere +a+. 
A ER +a+ representa um padrão que poderia ser descrito em português como 
``o conjunto de _strings_ que possuem um caractere +a+''. Obviamente só 
existe uma _string_ dessa forma: a _string_ +"a"+. Colocando um padrão 
após o outro realiza a _concatenação_ dos padrões. Começando com caracteres 
simples, se juntarmos um +a+ e um +b+ formamos a expressão +ab+, que 
representa a _string_ que contém um +a+ seguido por um +b+, ou seja, 
a _string_ +"ab"+.

(((*)))

Mas o poder das Expressões Regulares vem de alguns caracteres que não 
representam eles mesmos; esses são _caracteres especiais_. Um 
caractere especial bastante usado é o `*`, que representa zero ou mais 
repetições de um padrão. Por exemplo, a expressão +a*+ representa 
_strings_ com zero ou mais caracteres +a+. A _string_ vazia `""`
satisfaz esse padrão e corresponde a zero repetições; outras _strings_ 
satisfeitas pelo padrão são +"a"+, +"aa"+, +"aaa"+, etc. O asterisco 
representa zero ou mais repetições do padrão que vem antes, não só 
de um caractere: a expressão `(ab)*` representa `""`, +"ab"+, 
+"abab"+, +"ababab"+, etc. Mas pelas regras de precedência das 
expressões, `ab*` é o mesmo que `a(b*)`, que representa um +a+ 
seguido por zero ou mais caracteres +b+, e não é igual a `(ab)*`. 

(((|)))

Outro caractere especial importante é a barra vertical +|+, que representa 
opções nas partes de um padrão. Por exemplo, +a|b+ representa +a+ ou +b+, 
ou seja, as _strings_ +"a"+ e +"b"+. 

Isso nos leva ao exemplo apresentado antes: `a(a|b)*b` é uma expressão 
regular formada por três partes concatenadas em sequência: +a+, depois 
`(a|b)*` e por fim +b+. Isso significa que uma _string_ que satisfaz 
essa expressão deve começar com um caractere +a+, seguido por caracteres 
que satisfazem o padrão `(a|b)*` e terminando com um caractere +b+. O 
padrão `(a|b)*` é satisfeito por zero ou mais repetições do padrão +(a|b)+, 
que por sua vez é um padrão que é satisfeito por caracteres +a+ ou +b+. Ou 
seja, `(a|b)*` é um padrão que representa zero ou mais repetições de 
caracteres +a+ ou +b+. Alguns exemplos de cadeias que são representadas 
pela expressão `a(a|b)*b`:

* +"ab"+ (zero repetições do padrão interno `(a|b)*`)
* +"aab"+
* +"abb"+
* +"aabbbb"+
* +"abbaabab"+

===== Caracteres especiais `+` e `?`

(((+))) (((?)))

Ja vimos que o caractere especial `*` representa zero ou mais repetições de 
um padrão. O caractere especial `+` é similar, mas representa uma ou mais 
repetições; a única diferença é que o caractere `+` causa a obrigatoriedade 
de pelo menos uma repetição do padrão. A expressão `a+` representa as _strings_ 
`"a"`, `"aa"`, `"aaa"`, etc., sem incluir a _string_ vazia. 

O caractere especial `?` representa partes opcionais em um padrão, ou seja, 
zero ou uma repetição de um determinado padrão. A expressão `b?a+` representa 
_strings_ com uma ou mais repetições de +a+, podendo começar opcionalmente com 
um +b+. 

===== Classes de caracteres, intervalos e negação

(((Classes de caracteres)))

As classes de caracteres são uma notação adicional para representar opções de 
um caractere em um padrão. A classe +[abc]+ representa apenas um caractere, que 
pode ser +a+, +b+ ou +c+. Isso é o mesmo que a expressão `(a|b|c)`, e a notação 
de classes é apenas um atalho, principalmente quando existem várias opções. 

((([])))

A expressão `[0123456789]` representa um caractere que é um dígito numérico. 
Adicionando um caractere de repetição temos `[0123456789]+`, que representa 
_strings_ contendo um ou mais dígitos. Essas são exatamente as _strings_, 
como `"145"` ou `"017"`, que representam constantes inteiras. 

Quando uma classe inclui vários caracteres em uma sequência, como o exemplo 
anterior, podemos usar _((intervalos))_ para tornar as expressões mais compactas. 
Por exemplo, a expressão `[0123456789]` pode ser igualmente representada 
pelo intervalo `[0-9]`. A expressão `[a-z]` representa uma letra minúscula. 
Podemos usar vários intervalos em uma classe. Por exemplo, `[A-Za-z]` 
representa uma letra maiúscula ou minúscula, e `[0-9A-Za-z]` representa 
um dígito ou letra. Note que cada classe ainda representa apenas um caractere; 
os intervalos apenas criam novas opções para esse caractere. 

Algumas classes especiais podem ser usadas como abreviações. Por exemplo, 
`[:alpha:]` (((:alpha:))) representa um caractere alfabético (ou seja, é o mesmo que 
`[A-Za-z]`), e `[:alnum:]` representa um caractere alfabético ou um 
dígito. Outras classes especiais úteis são `[:space:]` ((([:space:]))) para caracteres 
de espaço em branco, `[:upper:]` ((([:upper:]))) para caracteres maiúsculos e `[:lower:]` ((([:lower:])))
para caracteres minúsculos. Existe a classe especial `[:digit:]` ((([:digit:]))) para dígitos,
mas em geral é mais compacto escrever `[0-9]`.
É importante lembrar que essas classes especiais, 
assim como os intervalos, só podem ser usados dentro de classes de caracteres, 
ou seja, não é possível ter uma expressão que seja apenas `[:alpha:]`; é 
preciso colocar a classe especial `[:alpha:]` dentro de uma classe, resultando 
na expressão `[[:alpha:]]`, que representa um caractere que pode ser qualquer 
letra (maiúscula ou minúscula).

IMPORTANT: Atenção ao uso dos colchetes (`[]`) nas classes especiais,
eles fazem parte da definição da classe. Quando utilizamos as classes
especiais dentro de um _intervalo_ nós teremos dois colchetes. Por
exemplo, a expressão para números de 0 a 7 ou letras maiúsculas é: `[0-7[:upper]]`.


Exemplo, para identificar números de 1 a 7 ou letras maiúsculas temos: `[0-7[:upper:]]`.

Uma outra notação útil com classes é a _((negação))_. Usar um caractere `^` (((`^`))) no começo 
de uma classe representa ``caracteres que não estão na classe''. Por exemplo, 
`[^0-9]` representa um caractere que não é um dígito de `0` a `9`. A negação 
também pode ser usada com classes especiais: `^[:alnum:]` representa um 
caractere que não é uma letra ou dígito. 

===== Metacaracteres e sequências de escape

Um outro tipo de caracteres especiais são os _((metacaracteres))_. Um metacaractere 
é um caractere especial que pode representar outros caracteres. O exemplo mais 
simples é o metacaractere (((.))) `.`, que pode representar qualquer caractere. A 
expressão `a.*b` representa _strings_ que começam com +a+, terminam com +b+ e 
podem ter qualquer número de outros caracteres no meio, por exemplo `"a0x13b"`. 

As sequências de escape são iguais as que existem na linguagem C: `\n` (((`\n`))) representa 
um caractere de nova linha, `\t` (((`\t`))) um caractere de tabulação, etc. A barra invertida (`\`) (((`\`)))
também pode ser usada para desativar a interpretação especial de um caractere 
especial. Por exemplo, se quisermos um caractere `+` em uma expressão regular que 
representa o símbolo de soma, e não a repetição de uma ou mais vezes, devemos usar 
`\+`. 

===== Outras características

((({})))

Além das possibilidades de repetição que vimos até agora (zero ou mais vezes, 
uma ou mais vezes, zero ou uma vez), é possível na notação do ((flex)) ser mais 
específico no _((número de repetições))_. Se `p` é um padrão, `p{2}` representa 
exatamente duas repetições do padrão, `p{2,}` representa duas ou mais 
repetições, e `p{2, 5}` representa um número de repetições entre duas 
e cinco, inclusive. A expressão `(la){3}` representa três repetições de `la`, 
ou seja, a _string_ `"lalala"`; e a expressão `(la){1,3}` representa as 
_strings_ `"la"`, `"lala"` e `"lalala"`.

TIP: Não vamos tratar aqui de todos os detalhes das expressões regulares no flex, 
mas eles podem ser consultados no manual da ferramenta em 
http://flex.sourceforge.net/manual/Patterns.html ou através do comando `info flex`
nos sistemas Unix.

===== Alguns exemplos

Agora que introduzimos a maior parte das características das expressões 
regulares no flex, veremos alguns exemplos de padrões descritos usando 
essa notação. Depois veremos outros exemplos diretamente ligados à 
análise léxica de linguagens de programação. 

* `[0-9]{3}\.[0-9]{3}\.[0-9]{3}\-[0-9]{2}` é um padrão 
que descreve os números de ((CPF)): três dígitos (`[0-9]{3}`) seguidos 
por um ponto (`\.`), depois mais três dígitos e um ponto, depois 
mais três dígitos, um hífen (`\-`) e finalmente dois dígitos.footnote:[Para verificar se o CPF realmente é válido ainda é necessário verificar os dígitos verificadores, o que não pode ser feito através de expresses regulares.]
 
* `[0-9]{2}\/[0-9]{2}\/[0-9]{4}` é um padrão que descreve 
((datas)) no formato DD/MM/AAAA com dois dígitos para dia e mês, e quatro dígitos 
para o ano. É preciso usar uma barra invertida `\` para incluir a barra `/` 
no padrão, caso contrário a barra seria interpretada como um caractere 
especial; no padrão, isso ocorre como `\/`. Esse padrão não verifica se a 
data é válida (uma __string__ como `"33/55/2033"` satisfaz o padrão).

* `[A-Z]{3}-[0-9]{4}` descreve ((placas)) de carro no Brasil, começando com 
três letras maiúsculas (`[A-Z]{3}`) seguidas por um hífen e quatro dígitos. 

* Os endereços de ((email)) seguem um conjunto de várias regras que estabelecem 
que caracteres podem ser usados (a maioria das regras pode ser encontrada 
nos RFCs 2821 e 2822). Um padrão simplificado para endereços de email pode 
ser o seguinte: `[[:alnum:]\._]+@[[:alnum:]]+\.[[:alnum:]]+` começando com 
um ou mais caracteres que podem ser letras, números, pontos e 
__underscore__ (a parte `[[:alnum:]\._]+`), seguidos pela arroba, depois 
uma ou mais letras ou dígitos, seguindo por um ponto e mais um grupo 
de uma ou mais letras ou dígitos. Esse padrão descreve um endereço 
de email simples como `nome@dominio.com`, mas não um endereço que tenha mais 
de um ponto após a arroba (por exemplo um email do Centro de Informática da 
UFPB, da forma `nome@ci.ufpb.br`). 

Agora que sabemos como especificar padrões de texto usando expressões regulares 
no flex, podemos usá-lo para gerar analisadores léxicos. 

=== Geradores de Analisadores Léxicos

Um gerador de analisadores léxicos é um programa que recebe como entrada a 
especificação léxica para uma linguagem, e que produz como saída um programa 
que faz a análise léxica para essa linguagem. Como vimos anteriormente, é 
possível escrever o código de um analisador léxico sem o uso de uma ferramenta, 
mas usar um gerador de analisadores é menos trabalhoso e tem maior garantia de 
gerar um analisador léxico correto. A <<gerador>> mostra um diagrama de 
blocos que descreve o uso de um gerador de analisadores (no caso o flex). O 
gerador recebe uma especificação da estrutura léxica na entrada, e gera um 
programa analisador (no caso do flex, um programa na linguagem C). Este 
programa, quando executado, recebe como entrada os caracteres do programa 
de entrada, e produz como saída a sequência de __tokens__ correspondentes. 

[[gerador]]
.Uso de um gerador de analisadores léxicos (flex).
image::images/lexica/gerador.eps[scaledwidth="40%"]

=== Uso do flex

No sistema Unix original foi criado um gerador de analisadores léxicos 
chamado *((lex))*, um dos primeiros geradores desse tipo. O projeto GNU criou 
o *((flex))* como uma versão do lex com licença de __software__ livre. 
O flex, assim como o lex original, é um gerador de analisadores léxicos 
que gera analisadores na linguagem C, e possui versões compatíveis nos 
principais sistemas operacionais atuais. 

Como mostrado na <<gerador>>, o flex recebe como entrada um arquivo 
de especificação e produz, na saída, um programa na linguagem C que implementa 
o analisador léxico que segue a especificação dada. Para usar o flex 
primeiramente precisamos saber como escrever a especificação léxica da linguagem 
no formato esperado pela ferramenta. 

==== Formato da entrada

A parte principal da especificação léxica no flex é um conjunto de regras. Cada 
regra é composta por duas partes: um padrão e uma ação; o padrão é uma expressão 
regular que descreve um determinado tipo de __token__ da linguagem, e a ação 
determina o que fazer quando encontrar o padrão correspondente. Para um compilador, 
a maioria das ações vai simplesmente criar um __token__ para o lexema 
encontrado, como veremos adiante. 

O formato do arquivo de especificação do flex é dividido em três partes separadas 
por uma linha contendo os caracteres `%%`, da seguinte forma:

.Formato de um arquivo de especificação do flex
----
definições
%%
regras
%%
código
----

A única parte obrigatória do arquivo são as (((flex,regras))) *regras*. 
As (((flex,definições))) *definições* permitem dar 
nomes a expressões regulares, o que é útil quando uma determinada expressão 
regular aparece como parte de vários padrões, ou como forma de documentação, 
para deixar mais claro o que significam as partes de um padrão complexo. 
Veremos exemplos de uso das definições mais adiante. No começo do arquivo 
tamém podem ser especificadas algumas opções que alteram o comportamento do 
analisador gerado. 

A terceira parte do arquivo pode conter (((flex,código))) *código* em linguagem C que será 
adicionado, sem alterações, ao programa C gerado pelo flex. Como a saída 
do flex é um programa em linguagem C, isso permite que o criador do 
arquivo de especificação adicione funções ou variáveis ao analisador gerado. 
Geralmente a parte de código é útil para definir funções auxiliares que 
podem ser usadas pelas ações. 

Já entendemos a maior parte do que é necessário para usar o flex, mas alguns 
detalhes só ficam claros com alguns exemplos. Vamos começar com um exemplo 
de especificação bastante simples. 

==== Uma especificação simples do flex

(((flex,especificação simples)))

Para exemplificar o uso do flex, vamos ver um exemplo de especificação simples 
e auto-contida que também serve como uma forma de testar os padrões do flex. 

:srcfile: simples.ll

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

[source,c]
.Especificação simples para o flex
----
%option noyywrap                            <1>

CPF [0-9]{3}\.[0-9]{3}\.[0-9]{3}-[0-9]{2}   <2>
EMAIL [[:alnum:]\._]+@[[:alnum:]]+\.[[:alnum:]]+

%%

{CPF}   { printf("CPF\n"); }                <3>
{EMAIL} { printf("EMAIL\n"); }
.       { printf("Caractere nao reconhecido\n"); }

%%

// funcao principal que chama o analisador  <4>
int main()
{
  yylex();
  return 0;
}
----
<1> Opção para não precisar criar uma função +yywrap+
<2> Definições
<3> Regras usando as definições
<4> Código: função +main+ para o programa do analisador 

A especificação contém as três partes: definições, regras e código. 
Antes das definições está especificada a opção (((noyywrap))) +noyywrap+, que 
simplifica a especificação (sem essa opção seria necessário escrever
uma função chamada (((yywrap))) +yywrap+). São definidas duas expressões, +CPF+ e 
+EMAIL+. 

Na parte de regras são descritas três, as duas primeiras determinam o 
que o programa deve fazer quando encontra um __token__ que satisfaz 
os padrões +CPF+ e +EMAIL+, e a terceira regra determina o que o 
programa deve fazer com __tokens__ que não satisfazem nenhum dos dois. 

A primeira regra é:

----
{CPF}   { printf("CPF\n"); }
----

Uma ação sempre tem duas partes, o padrão e a ação, separados por espaços 
em branco:

----
padrão ação
----

(((CPF)))

O padrão na primeira regra é especificado como +{CPF}+. O nome  
+CPF+ entre chaves especifica que deve ser usada a definição com esse 
nome, ao invés de tratar os caracteres como representando eles mesmos 
(o padrão +CPF+, sem chaves, seria satisfeito apenas pela __string__ 
`"CPF"`). A regra é um trecho de código C que será executado caso o 
padrão seja satisfeito pelo __token__. Nesse caso, apenas é impressa 
a __string__ `"CPF"`, para que o usuário veja o tipo de __token__ 
que foi determinado. 

O uso de definições é opcional. A seguinte regra tem exatamente o 
mesmo efeito que a regra anterior para números de CPF: 

----
[0-9]{3}\.[0-9]{3}\.[0-9]{3}-[0-9]{2}   { printf("CPF\n"); }
----

Veja que nesse caso não é preciso usar chaves ao redor do padrão. O uso 
de uma definição torna a especificação muito mais legível, deixando claro 
para o leitor o que a expressão regular representa. 

A regra para endereços de email segue os mesmos princípios. A terceira regra 
é:

----
.       { printf("Caractere nao reconhecido\n"); }
----

Como já vimos, o ponto é um metacaractere no flex que representa qualquer 
caractere. O padrão ((+.+)) (caractere ``.'') significa ``qualquer caractere'', ou seja, esse padrão 
reconhece qualquer caractere não reconhecido pelos padrões anteriores. 
É importante entender como as regras do flex são processadas: o analisador 
testa a __string__ atual com todos os padrões da 
especificação, procurando ver que padrões são satisfeitos pela __string__. 
Se mais de um padrão é satisfeito pelo __string__ ou parte dela, o analisador 
vai escolher o padrão que é satisfeito pelo maior número de caracteres. 

Usando as regras do exemplo atual, digamos que a __string__ atual seja um 
CPF. Essa __string__ satisfaz o padrão para números de CPF na primeira 
regra do arquivo de especificação, mas o primeiro caractere da __string__, 
que é um número, também satisfaz a última regra (o padrão `.`), pois 
esse padrão satisfaz qualquer caractere. Entre as duas regras ativadas, a
regra do CPF é casada com todos os caracteres da __string__ atual, enquanto 
que a regra do ponto só é casada com o primeiro caractere da __string__ 
atual. Portanto, a regra do CPF é casada com o maior número de caracteres, e 
essa regra é escolhida. Mas se a __string__ atual for uma sequência de 
três dígitos como +123+, o único padrão que é satisfeito é o último, do 
ponto, que aceita qualquer caractere, e nesse caso o analisador imprime 
mensagens de erro (o padrão do ponto é satisfeito três vezes por essa 
__string__, já que o ponto representa apenas um caractere).

[NOTE]
.Como o flex casa a entrada com os padrões
====
O funcionamento geral do flex é determinado pelos padrões que estão 
presentes nas regras especificadas para o analisador. Para a sequência 
de caracteres da entrada, o analisador gerado pelo flex tenta casar 
os caracteres de entrada, ou uma parte inicial deles, com algum padrão 
nas regras. 

Se apenas um padrão é casado com os caracteres iniciais da 
sequência, a regra de onde vem o padrão é __disparada__, ou seja, a 
ação da regra é executada pelo analisador. Se nenhum padrão for casado 
com os caracteres atuais, o analisador gerado executa uma regra 
padrão inserida pelo flex. A regra padrão simplesmente imprime na saída 
os caracteres não reconhecidos por nenhum padrão. 

Se mais de um padrão for satisfeito por uma sequência inicial dos 
caracteres atuais, o analisador escolhe o padrão que é casado com 
o maior número de caracteres e dispara a regra desse padrão. Se 
vários padrões casam com o mesmo número de caracteres da sequência 
atual, o analisador escolhe aquele que aparece primeiro no arquivo 
de especificação. Isso significa que a ordem das regras no arquivo 
de especificação é importante. 

Se uma regra é disparada ao casar os caracteres atuais com algum 
padrão, os caracteres restantes que não foram casados com o padrão
permanecem guardados para uma próxima vez que o analisador for 
chamado. Por exemplo, se a entrada é a __string__ +123456+ e 
o único padrão do analisador representa cadeias de três dígitos, 
a primeira chamada ao analisador vai casar os caracteres +123+ 
e disparar a regra associada, deixando os caracteres +456+ 
no analisador, para uma próxima chamada. Se o analisador for 
chamado novamente, a regra vai casar com os caracteres +456+ 
e disparar novamente. Dessa forma, o analisador pode atuar em 
um __token__ de cada vez. 
====

O código nesse caso define uma função +main+, para que o código C gerado pelo 
flex possa ser executado diretamente. A função +main+ apenas chama a função 
+yylex()+ (((+yylex()+))) que é a função principal do analisador léxico criado pelo flex. 
Todo arquivo C gerado pelo flex contém uma função +yylex+. O comportamento 
dessa função pode ser alterado de várias formas que veremos adiante, mas 
se chamada diretamente, da forma que fazemos nesse exemplo, ela funciona 
da seguinte forma: recebe __tokens__ na entrada padrão (lendo do teclado)
e executa as ações associadas para cada regra que é satisfeita; esse 
processo continua até que o fim de arquivo seja encontrado. Isso 
funciona bem como um forma de testar os padrões usados na especificação. 

Com o arquivo de especificação acima, podemos gerar o código C correspondente
chamando a ferramenta flex na linha de comando. Por convenção, os arquivos de 
especificação do flex têm extensão `.ll`. Depois de gerar um arquivo com código 
C, este pode ser compilado e executado diretamente (já que ele possui uma 
função +main+). Em um sistema Unix, a sequência de comandos é a seguinte: 

.Geração e compilação do arquivo C
....
$ flex -o simples.c simples.ll 
$ gcc -o simples simples.c 
....

Depois disso, o executável +simples+ vai funcionar como descrito: esperando 
entrada pelo teclado e imprimindo os tipos de __tokens__ reconhecidos:

.Exemplo de uso do analisador
....
$ ./simples 
111.222.333-99
CPF

nome@mail.com
EMAIL

123
Caractere nao reconhecido
Caractere nao reconhecido
Caractere nao reconhecido
....

Para terminar (((flex,terminar))) o teste, deve-se digitar o caractere de fim de arquivo (em 
sistemas Unix o ((fim de arquivo)) é entrado com Ctrl+D, enquanto em sistemas 
Windows deve-se usar Ctrl+Z).

Essa especificação pode ser usada para testar outros padrões. Ao mudar a 
especificação, deve-se gerar novamente o arquivo C usando o flex e compilar 
o arquivo C gerado. 

Em um compilador geralmente queremos que a entrada seja lida de um arquivo, 
e precisamos gerar os __tokens__ para cada lexema, não simplesmente imprimir 
o tipo de cada __token__ encontrado. Veremos nos próximos exemplos como fazer 
isso. 

==== Analisador léxico para expressões usando flex 

Vimos anteriormente um analisador léxico para uma linguagem de expressões 
criado diretamente na linguagem C, sem uso de gerador. Nosso próximo exemplo 
é um analisador para a mesma linguagem, mas agora usando flex. Isso serve a 
dois propósitos: o primeiro é mostrar mais algumas características do uso do 
flex; o segundo é comparar o esforço necessário para criar um analisador com 
e sem usar um gerador como o flex. 

O analisador é composto pelo arquivo de especificação, +exp.ll+, um arquivo C 
que chama o analisador léxico (+exp_flex.c+), e um arquivo de cabeçalho com 
definições (+exp_tokens.h+).

NOTE: Você pode consultar os códigos completos destes arquivos no <<app_code>>.

O arquivo de cabeçalho contém as definições de tipos e constantes, iguais ao 
analisador que foi mostrado anteriormente:

:srcfile: exp_flex/exp_tokens.h

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

O conteúdo principal desse arquivo é: 

[source, c]
.Arquivo de cabeçalho para analisador léxico de expressões
----
// constantes booleanas
#define TRUE            1
#define FALSE           0

// constantes para tipo de token
#define TOK_NUM         0 
#define TOK_OP          1
#define TOK_PONT        2
#define TOK_ERRO        3 

// constantes para valores de operadores
#define SOMA            0
#define SUB             1
#define MULT            2
#define DIV             3

// constantes para valores de pontuacao (parenteses)
#define PARESQ          0
#define PARDIR          1

// estrutura que representa um token
typedef struct 
{
  int tipo;
  int valor;
} Token; 

// funcao para criar um token
extern Token *token();

// funcao principal do analisador lexico
extern Token *yylex(); 
----

Além das constantes já vistas para tipo e valor do __token__, temos um novo tipo 
de __token__ declarado e protótipos para duas funções. O novo tipo é o 
+TOK_ERRO+, que sinaliza um erro na análise léxica. Esse tipo é usado quando o 
analisador recebe uma sequência de caracteres que não reconhece como __token__ da 
linguagem. As duas funções declaradas são a função principal do analisador, e 
uma função para criar __tokens__ que é usada pelo analisador. A função principal 
desse analisador retorna um ponteiro para uma estrutura +Token+, e por isso ela 
deve ser declarada de outra forma (a função ++yylex++ padrão retorna um inteiro, 
como vimos antes). 

Em seguida, vamos ver o arquivo de especificação para essa linguagem, que contém 
algumas novidades em relação ao que vimos antes: 

:srcfile: exp_flex/exp.ll

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

[source, c]
.Arquivo de especificação do flex para a linguagem de expressões
----
%option noyywrap                                  <1>
%option nodefault
%option outfile="lexer.c" header-file="lexer.h"

%top {                                            <2>
#include "exp_tokens.h"
}

NUM [0-9]+

%%

[[:space:]] { }  /* ignora espacos */

{NUM}   { return token(TOK_NUM,  atoi(yytext)); } <3>
\+      { return token(TOK_OP,   SOMA);   }
-       { return token(TOK_OP,   SUB);    }
\*      { return token(TOK_OP,   MULT);   }
\/      { return token(TOK_OP,   DIV);    }
\(      { return token(TOK_PONT, PARESQ); }
\)      { return token(TOK_PONT, PARDIR); } 

.       { return token(TOK_ERRO, 0);      }  //  erro para
                                             // token desconhecido
%%

// variavel global para um token                  <4>
Token tok;

Token * token(int tipo, int valor)
{
  tok.tipo = tipo;
  tok.valor = valor;
  return &tok;
}
----
<1> Opções: sem regra __default__, nomes dos arquivos gerados
<2> Trecho de código para incluir no topo do arquivo gerado
<3> Uso do lexema casado pelo padrão: a variável +yytext+
<4> Código para incluir no final do arquivo gerado

O arquivo de especificação usa todas as três partes: definições (e opções),
regras e código. Na parte de definição são incluídas as opções para não 
precisar da função +yywrap+ (((yywrap))) e opções para selecionar o nome dos arquivos
de saída. No exemplo anterior, o nome do arquivo de saída foi selecionado 
na linha de comando; nesta especificação usamos opções para não só 
selecionar o nome do arquivo C gerado, mas também garantir que será 
gerado um arquivo de cabeçalho com definições do analisador léxico (nesse 
caso, o arquivo +lexer.h+). 

A opção +nodefault+ (((nodefault))) deve ser usada para evitar que o flex inclua uma regra 
padrão (__default__) se nenhuma outra regra for satisfeita. A regra padrão 
do flex apenas mostra na saída os caracteres que não forem reconhecidos em 
alguma regra. Isso significa que erros léxicos no programa de entrada não 
serão reconhecidos, e que o analisador vai gerar saída desnecessária. 

Apenas uma definição é criada, a definição +NUM+ para constantes numéricas. 
A primeira regra ignora quaisquer caracteres de espaço (espaços, tabulações, 
caracteres de nova linha). Essa regra tem uma ação vazia, o que indica que 
os espaços devem ser apenas ignorados. As outras regras seguem a estrutura 
léxica da linguagem de expressões, que já vimos antes. O único cuidado 
adicional é que muitos dos caractere usados na linguagem são caracteres 
especiais no flex (`+`, `*`, barra e os parênteses) e portanto precisam 
ser incluídos no padrão com uma contrabarra antes. Cada ação apenas 
cria um __token__ com o tipo e o valor adequados, usando a função +token+ 
que veremos a seguir. 

A regra que cria __tokens__ de tipo número precisa obter o valor do número, 
e isso depende da sequência de caracteres que foi casada com o padrão. 
Ou seja, o analisador precisa ter acesso ao lexema que foi identificado para 
gerar o __token__. Analisadores gerados pelo flex incluem uma variável chamada
+yytext+ (((yytext))) que guarda os caracteres casados pelo padrão da regra que foi 
disparada. Por isso, a regra para __tokens__ de tipo número obtém o valor 
chamando a função +atoi+ da linguagem C na cadeia +yytext+, como visto na 
regra:

----
{NUM}   { return token(TOK_NUM,  atoi(yytext)); }
----

De resto, a última regra usa o padrão com um ponto para capturar erros 
léxicos na entrada. 

A seção de código inclui uma variável e uma função que serão incluídas
no analisador gerado. A variável +tok+ serve para guardar o __token__ 
atual, e a função +token+ serve para guardar os dados de um __token__ 
e retorná-lo para a parte do programa que chama o analisador. 

O arquivo C +exp_flex.c+ contém o programa principal que recebe a entrada 
do teclado e chama o analisador léxico. Esse arquivo contém as funções
+operador_str()+ e +imprime_token()+ que são idênticas às funções no 
arquivo +exp_lexer.c+ do analisador léxico anterior. A função principal 
do programa em +exp_flex.c+ é mostrada abaixo:

:srcfile: exp_flex/exp_flex.c

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

[source, c]
.Função principal do progra em +exp_flex.c+
----
int main(int argc, char **argv)
{
  char  entrada[200];
  Token *tok;

  printf("Analise Lexica para Expressoes\n");

  printf("Expressao: ");
  fgets(entrada, 200, stdin);

  inicializa_analise(entrada);

  printf("\n===== Analise =====\n");

  tok = proximo_token();
  while (tok != NULL) {
    imprime_token(tok);
    tok = proximo_token();
  }

  printf("\n");

  return 0;
}
----

A função principal é quase igual ao analisador criado diretamente, mas as funções 
+inicializa_analise()+ e +proximo_token()+ são diferentes. A função +proximo_token()+ 
apenas chama a função principal do analisador gerado pelo flex, +yylex()+:

[source, c]
.Função para obter o próximo token
----
Token *proximo_token() 
{
  return yylex();
}
----

E a função +inicializa_analise()+ chama uma função do analisador gerado que configura 
a análise léxica para ler de uma __string__ ao invés da entrada padrão. Para isso é 
preciso usar uma variável do tipo +YY_BUFFER_STATE+, que é um tipo declarado no 
analisador gerado. 

[source, c]
.Função que inicializa a análise léxica
----
YY_BUFFER_STATE buffer;

void inicializa_analise(char *str)
{
  buffer = yy_scan_string(str);
}
----

Para compilar o analisador são necessários dois passos, como antes: 

. Gerar o arquivo C do analisador usando o +flex+;
. Compilar os arquivos C usando o  compilador C;

Para gerar o analisador usando o flex, é preciso usar uma opção 
para determinar uma declaração diferente para a função principal 
do analisador, +yylex+:

.Comando para gerar os arquivos +lexer.c+ e +lexer.h+
....
flex -DYY_DECL="Token * yylex()" exp.ll
....

Isso vai gerar os arquivos +lexer.c+ e +lexer.h+. O passo seguinte é compilar 
o arquivo +lexer.c+ juntamente com o arquivo principal +exp_flex.c+. Usando 
o +gcc+ como compilador, a linha de comando seria: 

.Comandos para compilar os arquivos +lexer.c+ e +exp_flex.c+ gerando o executável `exp'
....
gcc -o exp lexer.c exp_flex.c
....

Isso gera um executável de nome +exp+. Quando executado, o programa funciona 
praticamente da mesma forma que o analisador 
criado diretamente para a mesma linguagem de expressões; a única diferença é o 
tratamento de erros. O analisador que usa o flex sinaliza os erros obtidos e 
continua com a análise.

Esse exemplo demonstra quase tudo que precisamos para fazer a análise léxica de 
uma linguagem de programação. O único detalhe que falta é saber como ler a 
entrada de um arquivo ao invés de uma __string__ ou da entrada padrão. 

==== Lendo um arquivo de entrada

A função +yylex+ gerada pelo flex normalmente lê sua entrada de um arquivo, a 
não ser que seja usada a função +yy_scan_string+ como no exemplo anterior. 
O analisador gerado pelo flex contém uma variável chamada +yyin+ (((yyin))) que é um 
ponteiro para o arquivo de entrada usado pelo analisador. Normalmente, essa 
variável é igual à variável global +stdin+ da linguagem C, ou seja, a 
entrada padrão. Para fazer que o analisador leia de um arquivo ao invés da 
entrada padrão, é necessário mudar o valor dessa variável para o arquivo 
desejado. 

O código necessário é basicamente o seguinte: 

.Exemplo de como fazer o analisador ler de um arquivo
----
int inicializa_analise(char *nome)
{
  FILE *f = fopen(nome, "r");
  
  if (f == NULL)
    return FALSE;

  // arquivo aberto com sucesso, direcionar analisador
  yyin = f;
}
----

Ou seja, deve-se abrir o arquivo desejado usando +fopen+, e depois 
atribuir a variável +yyin+ do analisador para apontar para o mesmo 
arquivo aberto. A partir daí, chamadas à função +yylex+ vão ler 
os caracteres usados no analisador do arquivo, ao invés da entrada 
padrão. É uma boa prática fechar o arquivo aberto durante a inicialização 
ao final da análise léxica. Vamos ver um exemplo de analisador que lê 
a entrada a partir de um arquivo a seguir. 

=== Análise Léxica de uma Linguagem de Programação 

Os exemplos vistos até agora foram preparação para sabermos como criar 
um analisador léxico para uma linguagem de programação. Vamos usar o 
flex como maneira mais fácil de criar um analisador do que escrever o 
código diretamente. Para especificar os padrões que definem os vários 
tipos de __tokens__ no flex, precisamos usar _definições regulares_. 
Já vimos como escrever um arquivo de especificação do flex para 
criar um analisador léxico, e como trabalhar com o código C gerado 
pelo flex. Nesta seção vamos juntar todas as peças e criar um 
analisador léxico para uma pequena linguagem de programação, uma 
versão simplificada da linguagem C. 

==== A Linguagem Mini C

A linguagem que vamos usar como exemplo é uma simplificação da linguagem 
de programação C, chamada aqui de Mini C. A ideia é que todo programa 
Mini C seja um programa C válido, mas muitas características da linguagem 
C não estão disponíveis em Mini C. 

Um programa Mini C é um conjunto de declarações de funções. Cada função 
é uma sequência de comandos. Alguns comandos podem conter expressões. 
Variáveis podem ser declaradas, apenas do tipo +int+. As estruturas de 
controle são apenas o condicional +if+ e o laço +while+. As expressões 
que podem ser formadas na linguagem também são simplificadas. 

Discutiremos mais sobre a sintaxe da linguagem Mini C no próximo capítulo. 
Aqui, o que interessa é a estrutura léxica da linguagem, ou seja, que tipos 
de __token__ ocorrem na linguagem e quais devem ser os valores associados 
a eles. Já que o analisador léxico é encarregado de retirar os comentários 
do código-fonte, também nesta fase precisamos definir a sintaxe para 
comentários. A linguagem C tem atualmente dois tipos de comentários: 
os comentários que podem se extender por múltiplas linhas, delimitados por 
`/*` e `*/`, e os comentários de linha única, que começam com `//` e vão 
até o final da linha. Vamos adotar esse último tipo de comentário na 
linguagem Mini C, pois ele é um pouco mais simples de tratar no analisador 
léxico. 

Os tipos de __tokens__ da linguagem Mini C são:

* identificadores (nomes de variáveis e funções)
* palavras-chave (+if+, +else+, +while+, +return+ e +printf+)
* constantes numéricas
* __strings__ (delimitadas por aspas duplas `"`)
* pontuação (parênteses, chaves, etc.)

Os identificadores devem ser iniciados por uma letra, seguida de zero ou mais 
letras ou dígitos. As constantes numéricas são formadas por um ou mais dígitos, 
e como pontuação incluímos as chaves `{` e `}`, os parênteses `(` e `)`, a 
vírgula e o ponto-e-vírgula. Nos operadores estão incluídos operadores 
aritméticos (as quatro operações básicas), comparações (a operação de menor e 
de igualdade), e dois operadores lógicos (E-lógico e negação).

Como a ideia é que os programas Mini C sejam compatíveis com compiladores C, 
todo programa Mini C deve poder ser compilado como se fosse um programa C. Os 
programas Mini C podem usar +printf+ para imprimir na tela (em Mini C, 
+printf+ é uma palavra-chave, não uma função como em C), e portanto para que 
isso não crie problemas com compiladores C, é preciso que os programas 
Mini C tenham a linha +#include <stdio.h>+ no começo. Por isso, um __token__ 
especial na linguagem Mini C é o chamado de __prólogo__, que é a __string__ 
+#include <stdio.h>+. 

Nos capítulos seguintes, veremos mais detalhes sobre a linguagem Mini C, mas 
neste capítulo vamos apenas trabalhar a estrutura léxica da linguagem. 

==== O analisador léxico para a linguagem Mini C

Aqui veremos um analisador léxico para a linguagem Mini C criado com o flex. 
Quase todas as características do flex usadas aqui já foram apresentadas 
antes, mas veremos algumas novidades. A maior novidade no analisador para 
a linguagem Mini C é a necessidade de usar tabelas para 
armazenar as __strings__ e os nomes de identificadores que ocorrem no 
programa.

Constantes para os tipos e valores de __tokens__ são definidos no arquivo de 
cabeçalho `minic_tokens.h`. 

:srcfile: minic/minic_tokens.h

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

A parte mais importante do conteúdo deste arquivo é mostrada abaixo:

[source, c]
.Definição de constantes para o analisador léxico
----
// Tipos de token
#define TOK_PCHAVE                              1
#define TOK_ID                                  4
#define TOK_NUM                                 5
#define TOK_PONT                                6
#define TOK_OP                                  7
#define TOK_STRING                              8
#define TOK_PROLOGO                             9
#define TOK_ERRO                                100

// valores para palavra-chave 
#define PC_IF                                   0
#define PC_ELSE                                 1
#define PC_WHILE                                2
#define PC_RETURN                               3
#define PC_PRINTF                               4

// valores para pontuacao
#define PARESQ                                  1
#define PARDIR                                  2
#define CHVESQ                                  3
#define CHVDIR                                  4
#define VIRG                                    5
#define PNTVIRG                                 6

// valores para operadores
#define SOMA                                    1
#define SUB                                     2
#define MULT                                    3
#define DIV                                     4
#define MENOR                                   5
#define IGUAL                                   6
#define AND                                     7
#define NOT                                     8
#define ATRIB                                   9

// tipos
typedef struct 
{
  int tipo;
  int valor;
} Token;
----

Agora vamos analisar o arquivo de especificação do flex para a linguagem 
Mini C, uma seção de cada vez. O arquivo completo pode ser encontrado no 
endereço abaixo. 

:srcfile: minic/lex.ll

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

A seção inicial contém opções e definições, além de trechos de código que 
são adicionados no começo do arquivo do analisador gerado:

[source, c]
.Opções e definições na especificação para o analisador Mini C
----
%option noyywrap 
%option nodefault
%option outfile="lexer.c" header-file="lexer.h"

%top {
#include "minic_tokens.h"
#include "tabelas.h"

// prototipo da funcao token
Token *token(int, int); 
}

NUM [0-9]+
ID [[:alpha:]]([[:alnum:]])*
STRING \"[^\"\n]*\"
----

As opções são as mesmas que já vimos no exemplo anterior. Temos um trecho de 
código incluído no começo do analisador (a parte começando com `%top`) que 
realiza a inclusão de dois arquivos de cabeçalho e define o protótipo da 
função `token` (definida na seção de código). Os cabeçalhos incluídos são 
`minic_tokens.h`, visto acima, e `tabelas.h`, que declara as funções para 
tabelas de __strings__ e de símbolos: as funções são chamadas de 
`adiciona_string` para adicionar uma nova __string__ na tabela, e 
`adiciona_simbolo` para um novo identificador na tabela de símbolos. 
Essas funções são definidas no arquivo `tabelas.c` e retornam o índice da 
__string__ ou símbolo na tabela respectiva; esse índice pode ser usado como 
valor do __token__. O uso de tabelas de __strings__ e de símbolos é importante 
por vários motivos, entre eles a eficiência do código do compilador. Em capítulos 
seguintes veremos como a tabela de símbolos é uma estrutura de importância 
central em um compilador. 

A especificação tem três definições: `NUM` é o padrão para constantes numéricas 
inteiras, idêntica a que vimos no exemplo anterior; `ID` é o padrão que especifica 
os identificadores da linguagem e `STRING` é o padrão que determina o que é uma 
literal __string__ em um programa Mini C: deve começar e terminar com aspas 
duplas, contendo no meio qualquer sequência de zero ou mais caracteres que não sejam 
aspas duplas nem caracteres de nova linha (`\n`). 

As regras da especificação são mostradas abaixo: 

[source, c]
.Regras na especificação para o analisador da linguagem Mini C
----
[[:space:]]  {  }  /* ignora espacos em branco */
\/\/[^\n]*     {  }  /* elimina comentarios */

"#include <stdio.h>"  { return token(TOK_PROLOGO, 0); }

{STRING} { return token(TOK_STRING, adiciona_string(yytext)); } 

if       { return token(TOK_PCHAVE, PC_IF); } 
else     { return token(TOK_PCHAVE, PC_ELSE); }
while    { return token(TOK_PCHAVE, PC_WHILE); }
return   { return token(TOK_PCHAVE, PC_RETURN); }
printf   { return token(TOK_PCHAVE, PC_PRINTF); } 

{NUM}    { return token(TOK_NUM, atoi(yytext)); }
{ID}     { return token(TOK_ID, adiciona_simbolo(yytext)); }

\+       { return token(TOK_OP, SOMA);  }
-        { return token(TOK_OP, SUB);   }
\*       { return token(TOK_OP, MULT);  }
\/       { return token(TOK_OP, DIV);   }
\<       { return token(TOK_OP, MENOR); } 
==       { return token(TOK_OP, IGUAL); }
&&       { return token(TOK_OP, AND);   }
!        { return token(TOK_OP, NOT);   }
=        { return token(TOK_OP, ATRIB); }

\(       { return token(TOK_PONT, PARESQ); }
\)       { return token(TOK_PONT, PARDIR); }
\{       { return token(TOK_PONT, CHVESQ); }
\}       { return token(TOK_PONT, CHVDIR); }
,        { return token(TOK_PONT, VIRG);   }
;        { return token(TOK_PONT, PNTVIRG); }

.        { return token(TOK_ERRO, 0);  }
----

A primeira regra ignora os espaços, como vimos no exemplo anterior. A segunda é 
para ignorar os comentários. Um comentário é qualquer sequência que começa com duas 
barras e vai até o fim da linha. Como a barra é um caractere especial no flex, ele 
precisa ser especificado com a contrabarra antes, e portanto `//` vira `\/\/` no 
padrão. As outras regras não têm novidade, exceto que a regra para __strings__ e 
a regra para identificadores obtêm o valor do __token__ chamando as funções 
`adiciona_string` e `adiciona_simbolo`, como discutimos antes. 

O analisador léxico normalmente não vai ser usado de forma isolada: ele é chamado 
pelo analisador sintático para produzir __tokens__ quando necessário, como veremos 
no próximo capítulo. Mas aqui vamos testar o funcionamento do analisador léxico 
isoladamente, criando um programa principal que abre um arquivo de entrada e 
chama o analisador léxico. Esse programa está no arquivo `lex_teste.c`:

:srcfile: minic/lex_teste.c

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

O programa principal é seguinte: 

[source, c]
.Função do principal do programa de teste do analisador léxico
----
int main(int argc, char **argv)
{
  Token *tok;

  if (argc < 2) {
    printf("Uso: ./mclex <arquivo>\n");
    return 0; 
  }

  inicializa_analise(argv[1]);

  tok = yylex();
  while (tok != NULL) {
    imprime_token(tok);
    tok = yylex();
  }

  finaliza_analise();

  return 0; 
}
----

A função principal obtém o nome do arquivo de entrada dos argumentos de linha 
de comando, inicializa a análise passando esse nome de arquivo, e para cada 
__token__ encontrado na entrada imprime uma representação desse __token__ na 
tela. A função `inicializa_analise` apenas tenta abrir o arquivo com o nome 
passado e configura o analisador léxico para usar esse arquivo, como discutimos
antes: 

[source, c]
.Função que inicializa a análise léxica
----
void inicializa_analise(char *nome_arq)
{
  FILE *f = fopen(nome_arq, "r");

  if (f == NULL) { 
    fprintf(stderr,"Nao foi possivel abrir o arquivo de entrada:%s\n",
       nome_arq);
    exit(1);
  }

  yyin = f;
}
----

E a função de finalização destrói as tabelas de símbolos e de __strings__, e 
fecha o arquivo de entrada: 

[source, c]
.Função de finalização da análise léxica
----
void finaliza_analise()
{
  // destroi tabelas
  destroi_tab_strings();
  destroi_tab_simbolos(); 

  // fecha arquivo de entrada
  fclose(yyin);
}
----

A função de impressão de __tokens__ apenas imprime o tipo e o valor do __token__, 
mas usando uma __string__ que representa o nome do tipo ao invés de simplesmente 
imprimir a constante numérica. A função completa pode ser vista no código fonte. 

Para testar o analisador, vamos criar um pequeno programa na linguagem Mini C. 
Esse programa pode ser encontrado no arquivo `teste.c`. 

:srcfile: minic/teste.c

.Código fonte
{online}/{srcfile}[{local}/{srcfile}]

[source, c]
.Arquivo de teste na linguagem Mini C
----
include::{local}/{srcfile}[]
----

Compilando o programa principal `lex_teste.c` para gerar um executável `mclex`, 
obtemos a seguinte saída para o arquivo `teste.c`: 

.Execução do analisador `mclex` sobre o arquivo `teste.c`
....
andrei$ ./mclex teste.c 
Tipo: prologo - Valor: 0
Tipo: identificador - Valor: 0
Tipo: identificador - Valor: 1
Tipo: pontuacao - Valor: 1
Tipo: pontuacao - Valor: 2
Tipo: pontuacao - Valor: 3
Tipo: palavra chave - Valor: 4
Tipo: pontuacao - Valor: 1
Tipo: string - Valor: 0
Tipo: pontuacao - Valor: 2
Tipo: pontuacao - Valor: 6
Tipo: palavra chave - Valor: 3
Tipo: numero - Valor: 0
Tipo: pontuacao - Valor: 6
Tipo: pontuacao - Valor: 4
....

NOTE: É interessante ver que o analisador ignorou a primeira linha com um comentário, e 
o primeiro __token__ mostrado é o prólogo. A sequência de __tokens__ segue corretamente 
do conteúdo do arquivo fonte. Também deve ser observado que o valor dos __tokens__ 
de tipo __string__ e identificador são os índices deles nas tabelas correspondentes. 
Para obter a __string__ ou identificador encontradas pelo analisador, é preciso 
acessar as tabelas. Veremos exemplos disso no capítulo seguinte. 

=== Conclusão

Neste capítulo, vimos o que é a etapa de análise léxica do compilador, e como 
criar um analisador léxico para qualquer linguagem de entrada. Vimos o que são 
os __tokens__ na análise léxica e como especificar os padrões que determinam os 
tipos de __tokens__ usando expressões regulares. É possível criar um analisador 
léxico escrevendo o código diretamente a partir dos padrões dos __tokens__, 
mas é mais prático usar um gerador de analisadores léxicos como o flex. Vimos o 
uso do flex para linguagens bastante simples e também para linguagens de 
programação mais próxima da realidade. Neste capítulo, também, vimos uma introdução 
à linguagem Mini C, que será usada como exemplo no resto do livro. Um analisador 
léxico completo para a linguagem Mini C, usando o flex, foi mostrado e detalhado 
neste capítulo. Esse analisador pode servir como base para criar analisadores 
para outros tipos de linguagens reais. 

No capítulo seguinte, veremos como utilizar a saída do analisador léxico para 
fazer a análise sintática dos programas, e estabelecer a estrutura sintática. 
Essa estrutura é de importância central na compilação dos programas. 





